{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgWQA9skY22S1vEzd1+EKs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raksha454/RDIY/blob/main/%20Voice_Emotion_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "Iq00_9bT8Wc_",
        "outputId": "e55760ef-c06b-4859-fc52-40c4f4f81d03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.3/1.8 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement umap-learn==0.5.9 (from versions: 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.1.4, 0.1.5, 0.2.0, 0.2.1, 0.2.2, 0.2.3, 0.2.4, 0.2.5, 0.3.0, 0.3.1, 0.3.2, 0.3.4, 0.3.5, 0.3.6, 0.3.7, 0.3.8, 0.3.9, 0.3.10, 0.4.0rc1, 0.4.0rc2, 0.4.0rc3, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.5, 0.4.6, 0.5.0rc1, 0.5.0, 0.5.1, 0.5.2, 0.5.3, 0.5.4, 0.5.5, 0.5.6, 0.5.7, 0.5.8, 0.5.9.post2)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for umap-learn==0.5.9\u001b[0m\u001b[31m\n",
            "\u001b[0mIt looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://83996510882ac0f39e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://83996510882ac0f39e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# =============================================\n",
        "# üß† Mental Health Voice Screening (Colab-Stable, Warning-Free)\n",
        "# =============================================\n",
        "\n",
        "# Step 1. Clean, compatible install\n",
        "!pip install -q --upgrade pip\n",
        "!pip install -q \\\n",
        "    gradio==3.39.0 \\\n",
        "    librosa==0.10.1 \\\n",
        "    soundfile==0.12.1 \\\n",
        "    numpy==1.26.4 \\\n",
        "    scikit-learn==1.6.0 \\\n",
        "    umap-learn==0.5.9 \\\n",
        "    tensorflow==2.16.1\n",
        "\n",
        "# Step 2. Disable analytics & suppress warnings\n",
        "import os, warnings\n",
        "os.environ[\"GRADIO_ANALYTICS_ENABLED\"] = \"False\"\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Step 3. Imports\n",
        "import gradio as gr\n",
        "import librosa\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Step 4. Download example audio\n",
        "!wget -q -O normal_voice.wav https://github.com/karoldvl/ESC-50/raw/master/audio/1-100032-A-0.wav\n",
        "!wget -q -O sad_voice.wav https://github.com/microsoft/EmotionSamples/raw/main/audio/sad1.wav\n",
        "\n",
        "# Step 5. Audio feature extraction\n",
        "def extract_features(audio_path):\n",
        "    y, sr = librosa.load(audio_path, sr=None)\n",
        "    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n",
        "    return mfcc\n",
        "\n",
        "# Step 6. Simple demo model (placeholder)\n",
        "model = Sequential([\n",
        "    Dense(16, activation='relu', input_shape=(40,)),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Step 7. Prediction logic\n",
        "def predict_mood(audio_path):\n",
        "    features = extract_features(audio_path).reshape(1, -1)\n",
        "    score = model.predict(features, verbose=0)[0][0]\n",
        "    if score > 0.5:\n",
        "        return \"‚ö†Ô∏è Possible Signs of Depression\"\n",
        "    return \"üôÇ Normal Mood Range\"\n",
        "\n",
        "# Step 8. Gradio interface\n",
        "desc = \"\"\"\n",
        "Upload or record a short voice clip.\n",
        "This demo extracts MFCC features and predicts emotional tone.\n",
        "‚ö†Ô∏è Research & educational use only ‚Äî not a diagnostic tool.\n",
        "\"\"\"\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=predict_mood,\n",
        "    inputs=gr.Audio(sources=[\"upload\", \"microphone\"], type=\"filepath\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"üß† Mental Health Voice Screening Demo\",\n",
        "    description=desc,\n",
        "    examples=[[\"normal_voice.wav\"], [\"sad_voice.wav\"]]\n",
        ")\n",
        "\n",
        "# Step 9. Launch the app\n",
        "iface.launch()\n"
      ]
    }
  ]
}